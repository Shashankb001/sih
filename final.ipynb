{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.93%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # Added for neural network\n",
    "from xgboost import XGBClassifier  # Added for XGBoost\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv('geo_points_Big.csv')\n",
    "\n",
    "# Function to convert time to numerical features\n",
    "def process_time(time_str):\n",
    "    time_parts = time_str.split(' ')[0].split(':')\n",
    "    hour, minute, second = map(int, time_parts)\n",
    "    am_pm = 0 if 'AM' in time_str else 1  # 0 for AM, 1 for PM\n",
    "    return hour, minute, second, am_pm\n",
    "\n",
    "# Apply the function to the Time column\n",
    "df[['Hour', 'Minute', 'Second', 'AM_PM']] = df['Time'].apply(lambda x: pd.Series(process_time(x)))\n",
    "\n",
    "# Create cyclical features for hour, minute, and second\n",
    "df['Hour_sin'] = np.sin(df['Hour'] * (2. * np.pi / 24))\n",
    "df['Hour_cos'] = np.cos(df['Hour'] * (2. * np.pi / 24))\n",
    "df['Minute_sin'] = np.sin(df['Minute'] * (2. * np.pi / 60))\n",
    "df['Minute_cos'] = np.cos(df['Minute'] * (2. * np.pi / 60))\n",
    "\n",
    "# Drop the original Time column and Count column (assuming it's not needed)\n",
    "df = df.drop(columns=['Time', 'Count', 'Hour', 'Minute', 'Second'])\n",
    "\n",
    "# Encode the Zone column to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "df['Zone'] = label_encoder.fit_transform(df['Zone'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate features and target variable from training dataset\n",
    "X_train = train.drop(columns='Zone')\n",
    "y_train = train['Zone']\n",
    "\n",
    "# Separate features and target variable from testing dataset\n",
    "X_test = test.drop(columns='Zone')\n",
    "y_test = test['Zone']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define different classifiers\n",
    "clf1 = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "# clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "# clf3 = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "# clf4 = SVC(kernel='linear', C=1, probability=True, random_state=42)\n",
    "clf5 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf6 = XGBClassifier(n_estimators=100, max_depth=3, random_state=42)  # Added XGBoost\n",
    "# clf7 = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)  # Added Neural Network\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(clf1, param_grid=param_grid_rf, cv=5, n_jobs=-1)\n",
    "grid_rf.fit(X_train_scaled, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "# # Hyperparameter tuning for Gradient Boosting\n",
    "# param_grid_gb = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'learning_rate': [0.1, 0.01],\n",
    "#     'max_depth': [3, 4],\n",
    "# }\n",
    "\n",
    "# grid_gb = GridSearchCV(clf2, param_grid=param_grid_gb, cv=5, n_jobs=-1)\n",
    "# grid_gb.fit(X_train_scaled, y_train)\n",
    "# best_gb = grid_gb.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for AdaBoost\n",
    "# param_grid_ada = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'learning_rate': [1.0, 0.5],\n",
    "# }\n",
    "\n",
    "# grid_ada = GridSearchCV(clf3, param_grid=param_grid_ada, cv=5, n_jobs=-1)\n",
    "# grid_ada.fit(X_train_scaled, y_train)\n",
    "# best_ada = grid_ada.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for SVM\n",
    "# param_grid_svc = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'kernel': ['linear', 'rbf'],\n",
    "# }\n",
    "\n",
    "# grid_svc = GridSearchCV(clf4, param_grid=param_grid_svc, cv=5, n_jobs=-1)\n",
    "# grid_svc.fit(X_train_scaled, y_train)\n",
    "# best_svc = grid_svc.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for KNN\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(clf5, param_grid=param_grid_knn, cv=5, n_jobs=-1)\n",
    "grid_knn.fit(X_train_scaled, y_train)\n",
    "best_knn = grid_knn.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(clf6, param_grid=param_grid_xgb, cv=5, n_jobs=-1)\n",
    "grid_xgb.fit(X_train_scaled, y_train)\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Neural Network\n",
    "# param_grid_nn = {\n",
    "#     'hidden_layer_sizes': [(100, 50), (200, 100)],\n",
    "#     'max_iter': [1000, 2000],\n",
    "#     'alpha': [0.0001, 0.001],\n",
    "# }\n",
    "\n",
    "# grid_nn = GridSearchCV(clf7, param_grid=param_grid_nn, cv=5, n_jobs=-1)\n",
    "# grid_nn.fit(X_train_scaled, y_train)\n",
    "# best_nn = grid_nn.best_estimator_\n",
    "\n",
    "# Update the ensemble with the best models\n",
    "eclf = VotingClassifier(estimators=[\n",
    "    # ('rf', best_rf),\n",
    "    # ('gb', best_gb),\n",
    "    # ('ada', best_ada),\n",
    "    # ('svc', best_svc),\n",
    "    ('knn', best_knn),\n",
    "    ('xgb', best_xgb)\n",
    "    # ('nn', best_nn)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model to the training data\n",
    "eclf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = eclf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# import joblib\n",
    "\n",
    "# class GeoZonePredictor:\n",
    "#     def __init__(self, model_file=None):\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.scaler = StandardScaler()\n",
    "#         if model_file:\n",
    "#             self.model = joblib.load(model_file)\n",
    "#         else:\n",
    "#             self.model = RandomForestClassifier()\n",
    "\n",
    "#     def preprocess(self, df):\n",
    "#         def process_time(time_str):\n",
    "#             time_parts = time_str.split(' ')[0].split(':')\n",
    "#             hour, minute, second = map(int, time_parts)\n",
    "#             am_pm = 0 if 'AM' in time_str else 1  # 0 for AM, 1 for PM\n",
    "#             return hour, minute, second, am_pm\n",
    "\n",
    "#         df[['Hour', 'Minute', 'Second', 'AM_PM']] = df['Time'].apply(lambda x: pd.Series(process_time(x)))\n",
    "#         df['Hour_sin'] = np.sin(df['Hour'] * (2. * np.pi / 24))\n",
    "#         df['Hour_cos'] = np.cos(df['Hour'] * (2. * np.pi / 24))\n",
    "#         df['Minute_sin'] = np.sin(df['Minute'] * (2. * np.pi / 60))\n",
    "#         df['Minute_cos'] = np.cos(df['Minute'] * (2. * np.pi / 60))\n",
    "#         df = df.drop(columns=['Time', 'Count', 'Hour', 'Minute', 'Second'])\n",
    "#         df['Zone'] = self.label_encoder.fit_transform(df['Zone'])\n",
    "#         return df\n",
    "\n",
    "#     def fit(self, df):\n",
    "#         df = self.preprocess(df)\n",
    "#         train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#         X_train = train.drop(columns='Zone')\n",
    "#         y_train = train['Zone']\n",
    "#         X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "#         self.model = VotingClassifier(estimators=self.clf_estimators, voting='soft')\n",
    "#         self.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#     def predict(self, df):\n",
    "#         df = self.preprocess(df)\n",
    "#         X_test = df.drop(columns='Zone')\n",
    "#         X_test_scaled = self.scaler.transform(X_test)\n",
    "#         predictions = self.model.predict(X_test_scaled)\n",
    "#         return predictions\n",
    "\n",
    "#     def evaluate(self, df):\n",
    "#         df = self.preprocess(df)\n",
    "#         X_test = df.drop(columns='Zone')\n",
    "#         y_test = df['Zone']\n",
    "#         X_test_scaled = self.scaler.transform(X_test)\n",
    "#         predictions = self.model.predict(X_test_scaled)\n",
    "#         accuracy = accuracy_score(y_test, predictions)\n",
    "#         print(f'Accuracy: {accuracy * 100:.2f}%')   \n",
    "\n",
    "#     def save_model(self, filepath):\n",
    "#         joblib.dump(self.model, filepath)\n",
    "\n",
    "#     def load_model(self, filepath):\n",
    "#         self.model = joblib.load(filepath)\n",
    "\n",
    "# # Usage:\n",
    "# # Assuming `best_knn` and `best_xgb` are your best models from your code above.\n",
    "# clf_estimators = [('knn', best_knn), ('xgb', best_xgb)]\n",
    "# predictor = GeoZonePredictor(clf_estimators)\n",
    "# df = pd.read_csv('geo_points_Big.csv')\n",
    "# predictor.fit(df)\n",
    "# predictor.evaluate(df)\n",
    "# predictor.save_model('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# import joblib\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# class GeoZonePredictor:\n",
    "#     def __init__(self, clf_estimators=None, model_file=None):\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.clf_estimators = clf_estimators\n",
    "#         if model_file:\n",
    "#             loaded_objects = joblib.load(model_file)\n",
    "#             self.model = loaded_objects['model']\n",
    "#             self.label_encoder = loaded_objects['label_encoder']\n",
    "#             self.scaler = loaded_objects['scaler']\n",
    "#         else:\n",
    "#             self.model = VotingClassifier(estimators=self.clf_estimators, voting='soft') if self.clf_estimators else None\n",
    "\n",
    "#     def preprocess(self, df):\n",
    "#         def process_time(time_str):\n",
    "#             time_parts = time_str.split(' ')[0].split(':')\n",
    "#             hour, minute, second = map(int, time_parts)\n",
    "#             am_pm = 0 if 'AM' in time_str else 1  # 0 for AM, 1 for PM\n",
    "#             return hour, minute, second, am_pm\n",
    "\n",
    "#         df[['Hour', 'Minute', 'Second', 'AM_PM']] = df['Time'].apply(lambda x: pd.Series(process_time(x)))\n",
    "#         df['Hour_sin'] = np.sin(df['Hour'] * (2. * np.pi / 24))\n",
    "#         df['Hour_cos'] = np.cos(df['Hour'] * (2. * np.pi / 24))\n",
    "#         df['Minute_sin'] = np.sin(df['Minute'] * (2. * np.pi / 60))\n",
    "#         df['Minute_cos'] = np.cos(df['Minute'] * (2. * np.pi / 60))\n",
    "#         df = df.drop(columns=['Time', 'Count', 'Hour', 'Minute', 'Second'])\n",
    "#         df['Zone'] = self.label_encoder.fit_transform(df['Zone'])\n",
    "#         return df\n",
    "\n",
    "#     def fit(self, df):\n",
    "#         if not self.clf_estimators:\n",
    "#             raise ValueError(\"Classifier estimators must be provided for training.\")\n",
    "#         df = self.preprocess(df)\n",
    "#         train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#         X_train = train.drop(columns='Zone')\n",
    "#         y_train = train['Zone']\n",
    "#         X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "#         self.model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#     def predict(self, df):\n",
    "#         df = self.preprocess(df)\n",
    "#         X_test = df.drop(columns='Zone')\n",
    "#         X_test_scaled = self.scaler.transform(X_test)\n",
    "#         predictions = self.model.predict(X_test_scaled)\n",
    "#         return predictions\n",
    "\n",
    "#     def evaluate(self, df):\n",
    "#         df = self.preprocess(df)\n",
    "#         X_test = df.drop(columns='Zone')\n",
    "#         y_test = df['Zone']\n",
    "#         X_test_scaled = self.scaler.transform(X_test)\n",
    "#         predictions = self.model.predict(X_test_scaled)\n",
    "#         accuracy = accuracy_score(y_test, predictions)\n",
    "#         print(f'Accuracy: {accuracy * 100:.2f}%')   \n",
    "\n",
    "#     def save_model(self, filepath):\n",
    "#         joblib.dump({'model': self.model, 'label_encoder': self.label_encoder, 'scaler': self.scaler}, filepath)\n",
    "\n",
    "#     def load_model(self, filepath):\n",
    "#         loaded_objects = joblib.load(filepath)\n",
    "#         self.model = loaded_objects['model']\n",
    "#         self.label_encoder = loaded_objects['label_encoder']\n",
    "#         self.scaler = loaded_objects['scaler']\n",
    "\n",
    "# # Usage:\n",
    "# # Define the classifiers and hyperparameters for grid search\n",
    "# knn_params = {'n_neighbors': [3, 5, 7]}\n",
    "# xgb_params = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "# knn = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n",
    "# xgb = GridSearchCV(XGBClassifier(), xgb_params, cv=5)\n",
    "\n",
    "# clf_estimators = [('knn', knn), ('xgb', xgb)]\n",
    "# predictor = GeoZonePredictor(clf_estimators)\n",
    "# df = pd.read_csv('geo_points_Big.csv')\n",
    "# predictor.fit(df)\n",
    "# predictor.evaluate(df)\n",
    "# predictor.save_model('model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.16%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "class GeoZonePredictor:\n",
    "    def __init__(self, model_file=None):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        if model_file:\n",
    "            loaded_objects = joblib.load(model_file)\n",
    "            self.model = loaded_objects['model']\n",
    "            self.label_encoder = loaded_objects['label_encoder']\n",
    "        else:\n",
    "            self.model = None  # Model will be defined in the fit method\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        def process_time(time_str):\n",
    "            time_parts = time_str.split(' ')[0].split(':')\n",
    "            hour, minute, second = map(int, time_parts)\n",
    "            am_pm = 0 if 'AM' in time_str else 1  # 0 for AM, 1 for PM\n",
    "            return hour, minute, second, am_pm\n",
    "\n",
    "        df[['Hour', 'Minute', 'Second', 'AM_PM']] = df['Time'].apply(lambda x: pd.Series(process_time(x)))\n",
    "        df['Hour_sin'] = np.sin(df['Hour'] * (2. * np.pi / 24))\n",
    "        df['Hour_cos'] = np.cos(df['Hour'] * (2. * np.pi / 24))\n",
    "        df['Minute_sin'] = np.sin(df['Minute'] * (2. * np.pi / 60))\n",
    "        df['Minute_cos'] = np.cos(df['Minute'] * (2. * np.pi / 60))\n",
    "        df = df.drop(columns=['Time', 'Count', 'Hour', 'Minute', 'Second'])\n",
    "        df['Zone'] = self.label_encoder.fit_transform(df['Zone'])\n",
    "        return df\n",
    "\n",
    "    def fit(self, df):\n",
    "        df = self.preprocess(df)\n",
    "        train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        X_train = train.drop(columns='Zone')\n",
    "        y_train = train['Zone']\n",
    "\n",
    "        # Create a pipeline with feature scaling, feature selection, and classification\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', MinMaxScaler()),  # Use MinMaxScaler instead of StandardScaler\n",
    "            ('feature_selection', SelectKBest(score_func=chi2, k='all')),  # Or use score_func=f_classif\n",
    "            ('classifier', RandomForestClassifier())\n",
    "        ])\n",
    "\n",
    "        # Define the grid of hyperparameters to search\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "\n",
    "        # Use GridSearchCV to find the best hyperparameters\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.model = grid_search.best_estimator_\n",
    "\n",
    "    def predict(self, df):\n",
    "        df = self.preprocess(df)\n",
    "        X_test = df.drop(columns='Zone')\n",
    "        predictions = self.model.predict(X_test)\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, df):\n",
    "        df = self.preprocess(df)\n",
    "        X_test = df.drop(columns='Zone')\n",
    "        y_test = df['Zone']\n",
    "        predictions = self.model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')   \n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        joblib.dump({'model': self.model, 'label_encoder': self.label_encoder}, filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        loaded_objects = joblib.load(filepath)\n",
    "        self.model = loaded_objects['model']\n",
    "        self.label_encoder = loaded_objects['label_encoder']\n",
    "\n",
    "# Usage:\n",
    "predictor = GeoZonePredictor()\n",
    "df = pd.read_csv('geo_points_Big.csv')  # Assuming your data is in this file\n",
    "predictor.fit(df)\n",
    "predictor.evaluate(df)\n",
    "predictor.save_model('model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 194ms/step\n",
      "Predicted Zone: red\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# # Load your CSV data with numerical zone values\n",
    "# df = pd.read_csv('geo_points_Big.csv')  # Replace 'your_data.csv' with your actual file name\n",
    "\n",
    "# # Extract features (latitude and longitude) and labels (zone)\n",
    "# X = df[['Latitude', 'Longitude']].values\n",
    "# y = df['Zone'].values\n",
    "\n",
    "# # Encode the zone labels (0, 1, 2)\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the input features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Define a TensorFlow neural network model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(2,)),  # Input layer for latitude and longitude\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer\n",
    "#     tf.keras.layers.Dense(32, activation='relu'),  # Hidden layer\n",
    "#     tf.keras.layers.Dense(3, activation='softmax')  # Output layer with 3 classes (zones)\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model on the test data\n",
    "# test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "# print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save('zone_prediction_model')\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_train contains your training data with latitude and longitude\n",
    "mean_latitude = np.mean(X_train[:, 0])\n",
    "mean_longitude = np.mean(X_train[:, 1])\n",
    "stddev_latitude = np.std(X_train[:, 0])\n",
    "stddev_longitude = np.std(X_train[:, 1])\n",
    "\n",
    "# Use these values for standardization during training and save them\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('zone_prediction_model')\n",
    "\n",
    "# Standardize the input features using TensorFlow\n",
    "sample_latitude = 10.7128  # Replace with your sample latitude\n",
    "sample_longitude = 4.0060  # Replace with your sample longitude\n",
    "\n",
    "sample_data_scaled = tf.constant(\n",
    "    [[(sample_latitude - mean_latitude) / stddev_latitude, (sample_longitude - mean_longitude) / stddev_longitude]],\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Predict the zone for the sample data point\n",
    "predictions = model.predict(sample_data_scaled)\n",
    "\n",
    "# Decode the predicted label back to the original zone label using a list of class labels\n",
    "class_labels = ['black', 'orange', 'red']  # Define the class labels in the same order as your model's output\n",
    "predicted_zone = class_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f\"Predicted Zone: {predicted_zone}\")\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_user_location():\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    try:\n",
    "        user_location = geolocator.geocode(\"Your Address Here\")  # Replace with user's address or leave it empty\n",
    "        if user_location:\n",
    "            latitude = user_location.latitude\n",
    "            longitude = user_location.longitude\n",
    "            return latitude, longitude\n",
    "    except Exception as e:\n",
    "        print(\"Error: Unable to get user's location.\")\n",
    "    return None, None\n",
    "\n",
    "# Get the user's current location\n",
    "user_latitude, user_longitude = get_user_location()\n",
    "print(user_latitude)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
